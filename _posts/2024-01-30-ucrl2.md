---
title: "Note: (UCRL2) Near-optimal Regret Bounds for Reinforcement Learning"
date: 2024-01-30 20:56:08 -0700
category: Note
comments: true
share: true
related: false
mathjax: true
---

## Problem setting

In a Markov decision process (MDP) $$M$$ with finite state space $$\mathcal{S}$$ and finite action space $$\mathcal{A}$$. The learner in some state $$s\in\mathcal{S}$$ chooses an action $$a\in\mathcal{A}$$ and receives a random reward $$r$$ drawn independently from a distribution on $$[0,1]$$ with mean $$\bar{r}(s,a)$$.

The MDP considered in UCRL2 is fully communicating, which means that starting from one state $$s$$, it only costs a finite time step to return it. Formally, let

\\\[D(M) := \max_{s\neq s'\in\mathcal{S}} \min_{\pi : \mathcal{S}\rightarrow \mathcal{A}} \mathbf{E}[T(s'\mid M, \pi, s)]\\]

$$D(M)$$ is called the "diameter" of the MDP $$M$$. $$M$$ is fully communicating representing that $$M$$ is finite number.

## Algorithm

First, I give a short summary about the UCRL2. In each episode $$k$$:

1.  UCRL2 first collects all history statistics $$(\{s_t, a_t, r_t\}_{t=1}^{t_k})$$.

2. Construct a confidence set $$\mathcal{M}_k$$ including true MDP $$M$$ with high probability. Every MDP in $$\mathcal{M}$$ should satisfy the following conditions.

   
   $$
   \begin{align}
   |\tilde{r}(s,a) - \hat{r}_k(s,a)| \lesssim& \sqrt {\frac{\log(S A t_k / \delta)}{N_+(s,a)}}
   	\label{cond:reward}
   \\
   ||\tilde{p}(\cdot|s,a) - \hat{p}_k(\cdot|s,a)||_1 \lesssim& \sqrt {\frac{\log(A t_k / \delta)}{N_+(s,a)}}
   	\label{cond:prob}
   \end{align}
   $$
   

3. Pick the most optimal MDP $$M^k$$ from the confident set $$\mathcal{M}_k$$ and use extended value iteration to find a policy $$\tilde{\pi}^k$$.

4. Execute policy $$\tilde{\pi}^k$$ and collect statistics $$(\{s_t, a_t, r_t\}_{t=t_k+1}^{t_{k+1}})$$.

The UCRL2 algorithm will repeat the above steps until the stopping creteria has been satisfied.

## Theorem Results

Theoretically, if the agent follows the optimal policy $$\pi^*(M)$$ in the MDP $$M$$. the regret of the algorithm UCRL2 is


$$
\text{Regret}(M, \text{UCRL2}, s, T) := T \rho^*(M) - R(M, \text{UCRL2}, s, T)
$$


Where $$\rho^*(M)$$ is the average reward by running optmal policy.

**Theorem 2 (Problem independent bound)** With probability of at least $$1-\delta$$ it holds that for any initial state $$s\in\mathcal{S}$$ and any $$T>1$$, the regret of UCRL2 is bounded by


$$
\text{Regret}(M, \text{UCRL2}, s, T) \leq 34 \cdot DS \sqrt{AT \log(\frac{T}{\delta})}
$$


Where $$S=\mid \mathcal{S} \mid$$, $$A=\mid\mathcal{A}\mid$$ and $$D$$ is the diameter of $$M$$.

**Theorem 3 (Problem dependent bound)** With probability of at least $$1-\delta$$ it holds that for any $$\varepsilon > 0$$, any initial state $$s\in\mathcal{S}$$ and any $$T>1$$, the regret of UCRL2 is bounded by

\\[ \text{Regret}(M, \text{UCRL2}, S, T) \leq 34^2 \cdot \frac{D^2 S^2 A \log(\frac{T}{\delta})}{\varepsilon} + \varepsilon T,\\]

Where $$S=\mid \mathcal{S} \mid$$, $$A=\mid\mathcal{A}\mid$$ and $$D$$ is the diameter of $$M$$.



## Regret Analysis



### The proof of theorem 2

#### Spiltting into episodes

The total regret can be decomposed as the sum of the regret accumulated in the individual episodes. That is


$$
\text{Regret}(M, \text{UCRL2}, S, T)= \sum_{k=1}^K \Delta_k.
$$


For each individual episode $$k$$, its regret is


$$
\Delta_k := v_k(s,a) (\rho^* - \bar{r}(s,a))
$$


where $$v_k(s,a)$$ is the final counts of state-action pair $$(s,a)$$ in episode $$k$$. We can regard $$\Delta_k$$ as the accumulate regret generated from every decision made in that episode. Each decision made in the state $$s$$ taking action $$a$$ will contribute a $$(\rho^*(s) - \bar{r}(s,a))$$ which is the difference between the optimal value and the mean value generated by taking action $$a$$ from state $$s$$.

#### Dealing with Failing Confidence Regions

In each epsido-regret $$\Delta_k$$, we can further decompose it into two parts, one is the confidence set contains the true $$MDP$$, otherwise it is not.


$$
\begin{align} 
\sum_{k=1}^K \Delta_k 
= 
\underbrace{\sum_{k=1}^K \Delta_k 1[ M\in\mathcal{M_k} ]}_{\text{Follwing Confidence Regions}} + \underbrace{\sum_{k=1}^K \Delta_k 1[ M\notin\mathcal{M_k} ]}_{\text{Failing Confidence Regions}}
\end{align}
$$


The first case we have a confidence set that constraints all reward function $$\tilde{r}(s,a)$$ and transition probability $$\tilde{p}(\cdot\mid s,a)$$ simultaneously. When $$M$$ is not contained in the confidence set $$\mathcal{M}_k$$, the regret bound should be a lower order than the first one.


$$
\begin{align*} 
\sum_{k=1}^K \Delta_k 1[M\notin\mathcal{M}_k] 
\leq& 
\sum_{k=1}^K \sum_{s,a} v_k(s,a)1[M\notin\mathcal{M}_k] 
\\
\leq&
\sum_{k=1}^K t_k \cdot1[M\notin\mathcal{M}_k] 
\\
=&
\sum_{k=1}^K \sum_{i=1}^T t_k \cdot1[t_k=i, M\notin\mathcal{M}_k] 
\\
=&
\sum_{i=1}^T i \sum_{k=1}^K 1[t_k=i, M\notin\mathcal{M}_k] 
\\
\leq&
\sum_{i=1}^T i 1[M\notin\mathcal{M}_k]
\\
=&
\sum_{i=1}^{T^\frac{1}{4}} i 1[M\notin\mathcal{M}_k]
+
\sum_{i=T^\frac{1}{4}+1}^{T} i 1[M\notin\mathcal{M}_k]
\\
\leq&
\sqrt{T} + \sqrt{T}
\end{align*}
$$


One step has not been clarified in above is the last step where $$\sum_{i=T^\frac{1}{4}+1}^{T} i 1[M\notin\mathcal{M}_k] \leq \sqrt{T} $$.

#### Dealing with Follwing Confidence Regions

In the case where $$M \in \mathcal{M}_k$$, the optimixtic average reward $$\tilde{\rho}_k$$ chosen by policy $$\tilde{\rho}_k$$ is essentially larger than the true optimal average reward $$\rho^*$$. Then for the accumulated regret in the $$k$$-th episode.


$$
\begin{align*}
\Delta_k 1[M\in\mathcal{M}_k] 
=& 
\sum_{s,a} v_k(s,a)(\rho^* - \bar{r}(s,a)) 1[M\in\mathcal{M}_k]
\\
\leq& 
\sum_{s,a} v_k(s,a)(\tilde{\rho}_k+\frac{1}{\sqrt{t_k}} - \bar{r}(s,a)) 1[M\in\mathcal{M}_k]
\\
=& 
\sum_{s,a} v_k(s,a)(\tilde{\rho}_k - \bar{r}(s,a))1[M\in\mathcal{M}_k]
+ \sum_{s,a} \frac{v_k(s,a)}{\sqrt{t_k}} 
\end{align*}
$$


The inequality is from the condition that $$M \in \mathcal{M}_k$$, thus $$\rho^* \leq \tilde{\rho}_k + \frac{1}{\sqrt{t_k}}$$. Then we can further subtract and add an $$\tilde{r}_k(s, \tilde{\pi}_k(s))$$ in the above equation


$$
\begin{align*}
& \Delta_k 1[M\in\mathcal{M}_k]
\\
\leq& \sum_{s,a} v_k(s,a)(\tilde{\rho}_k - \bar{r}(s,a))1[M\in\mathcal{M}_k] + \sum_{s,a} \frac{v_k(s,a)}{\sqrt{t_k}}
\\
=& 
\sum_{s,a} v_k(s,a)(\tilde{\rho}_k - \tilde{r}_k(s,a) + \tilde{r}_k(s,a) - \bar{r}(s,a))1[M\in\mathcal{M}_k]
+ \sum_{s,a} \frac{v_k(s,a)}{\sqrt{t_k}}
\\
=&
\sum_{s,a} v_k(s,a)(\tilde{\rho}_k  - \tilde{r}_k(s,a))1[M\in\mathcal{M}_k]
+ 
\sum_{s,a} v_k(s,a) (\tilde{r}_k(s,a) - \bar{r}(s,a)) 1[M\in\mathcal{M}_k]
+ 
\sum_{s,a} \frac{v_k(s,a)}{\sqrt{t_k}}
\\
\leq&
\underbrace{ \mathbf{v_k}(\tilde{P}_k - I) \mathbf{u_i}1[M\in\mathcal{M}_k] }_{\mathrm{I}}
+ 
\underbrace{\sum_{s,a} v_k(s,a) (\tilde{r}_k(s,a) - \bar{r}(s,a))1[M\in\mathcal{M}_k]}_{\mathrm{II}} 
+ 
2 \sum_{s,a} \frac{v_k(s,a)}{\sqrt{t_k}}
\end{align*}
$$


**The first term I:** According to the condition bounding probability transition, the first term can be controlled by the concentration inequality


$$
\begin{align}
&
\mathbf{v}_k(\tilde{P}_k - I)\mathbf{u_i}1[M\in\mathcal{M}_k]
\\
=&
 \mathbf{v_k}(\tilde{P}_k - I) \mathbf{u_i}1[M\in\mathcal{M}_k]  + 
\mathbf{v}_k(\tilde{P}_k - I)(\mathbf{u_i}-\mathbf{w_k})1[M\in\mathcal{M}_k]
\\
=&
\mathbf{v}_k(\tilde{P}_k - P_k)\mathbf{w_k} 1[M\in\mathcal{M}_k]
+ \mathbf{v}_k(P_k - I)\mathbf{w_k} 1[M\in\mathcal{M}_k]
+ \sum_{s,a} \mathbf{v}_k(\tilde{P}_k - I)(\mathbf{u_i}-\mathbf{w_k}) 1[M\in\mathcal{M}_k]
\end{align}
$$


where  $$w_k(s) := u_i(s) - \frac{\min_s u_i(s) + \max_s u_i(s)}{2}$$ and $$\mid\mid w_k \mid\mid \leq \frac{D}{2}$$. The first sum can be controlled by the definition of $$\mathcal{M}_k$$ where the variation of the probability transition matrix is upper bounded. The second sum can be regarded as a sum of martingle and can be controlled by \textbf{Azuma-Hoeffding inequality} since we can treat each variable $$X_t := (p(\cdot \mid s_t,a_t) - e_{s_{t+1}}) w_{k(t)} 1[M\in\mathcal{M}_{k(t)}]$$, where $$k(t)$$ represents the episode contains step $$t$$. The third sum is equal to $$0$$ because the sum of rows of $$\tilde{P}_k$$ is $$1$$.

Therefore, for the term I we have


$$
\begin{align}
&\sum_{s,a} v_k(s,a)(\tilde{\rho}_k - \tilde{r}_k(s,a))
\\
\leq& 
D\sqrt{14S \log(\frac{2AT}{\delta})} \cdot \sum_{k=1}^{m}\sum_{s,a} \frac{v_k(s,a)}{\sqrt{N_+(s,a)}}
+ \sum_{t=1}^T X_t + mD
+ \sum_{s,a}\frac{v_k(s,a)}{\sqrt{t_k}}
\end{align}
$$


**The second term II:** Since it is the sum of reward difference between the reward obtained by executing empirical optimal policy $$\tilde{\pi}_k$$, $$\tilde{r}_k(s,a)$$, to the mean reward $$\bar{r}_k(s,a)$$. We can use $$\hat{r}_k(s,a)$$ as the bridge since the condifence set $$\mathcal{M}_k$$ is constructed centered around $$\hat{r}_k(s,a)$$. Both $$\tilde{r}_k(s,a)$$ and $$\bar{r}_k(s,a)$$ should satisfy the reward confidence condition.


$$
\begin{align}
&\sum_{s,a} v_k(s,a) (\tilde{r}_k(s,a) - \bar{r}(s,a))
\\
=&
\sum_{s,a} v_k(s,a) (\tilde{r}_k(s,a) - \hat{r}_k(s,a) + \hat{r}_k(s,a) - \bar{r}(s,a))
\\
\leq&
\sum_{s,a} v_k(s,a) |\tilde{r}_k(s,a) - \hat{r}_k(s,a)| + 
\sum_{s,a} v_k(s,a) |\hat{r}_k(s,a) - \bar{r}(s,a))|
\\
\lesssim&
2\sum_{s,a} v_k(s,a) \sqrt{\frac{\log(2S A t_k/\delta)}{N_+(s,a)}}
\end{align}
$$


Overall, we can add up those upper bound and get


$$
\begin{align}
&  \sum_{k=1}^m \Delta_k 1[M\in\mathcal{M}_k]
\\
\lesssim&
D\sqrt{T\log(\frac{8T}{\delta})} + DSA \log(\frac{8T}{SA}) + D\sqrt{S\log(\frac{2AT}{\delta})}\sqrt{SAT}
\\
\lesssim&
DS\sqrt{AT \log(\frac{AT}{\delta})}
\end{align}
$$
